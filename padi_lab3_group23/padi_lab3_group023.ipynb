{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ak1esab9bjcx"
   },
   "source": [
    "# Learning and Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFcrWp2tbjcz"
   },
   "source": [
    "## Laboratory 3: Partially observable Markov decision problems\n",
    "\n",
    "In the end of the lab, you should export the notebook to a Python script (``File >> Download as >> Python (.py)``). Make sure that the resulting script includes all code written in the tasks marked as \"**Activity n. N**\", together with any replies to specific questions posed. Your file should be named `padi-labKK-groupXXX.py`, where `KK` corresponds to the lab number and the `XXX` corresponds to your group number. Similarly, your homework should consist of a single pdf file named `padi-hwKK-groupXXX.pdf`. You should create a zip file with the lab and homework files and submit it in Fenix **at most 30 minutes after your lab is over**.\n",
    "\n",
    "Make sure to strictly respect the specifications in each activity, in terms of the intended inputs, outputs and naming conventions.\n",
    "\n",
    "In particular, after completing the activities you should be able to replicate the examples provided (although this, in itself, is no guarantee that the activities are correctly completed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqqTlZBvbjcz"
   },
   "source": [
    "### 1. The POMDP model\n",
    "\n",
    "Consider the following POMDP problem. It describes the decision process of a police chasing a bandit (like a 1d scotland yard game). The game proceeds in rounds. At each round,\n",
    "\n",
    "* The police can go either left, right, observe or try to catch the bandit.\n",
    "* If the bandit is in the same location as the police, the police catches the bandit, gets a cost of zero and a new game starts with the police at position 0 and the bandit at position 2.\n",
    "* If the bandit is not in the same location as the police, the police receives a penalty/cost.\n",
    "* At each step (except when the bandit is caught) the bandit has a probability of moving left, right, or staying in the same location.\n",
    "* The position of the bandit can only be seen when police uses action observe; when used, the police perceives the correct location of the bandit (irrespective of the location of the police). Observations are '0' '1' '2' '3' '4' for bandit detect in location the respective location, or 'e' for empty.\n",
    "\n",
    "\n",
    "In this lab you will use a POMDP based on the aforementioned domain and investigate how to simulate a partially observable Markov decision problem and track its state. You will also compare different MDP heuristics with the optimal POMDP solution.\n",
    "\n",
    "**Throughout the lab, unless if stated otherwise, use $\\gamma=0.99$.**\n",
    "\n",
    "$$\\diamond$$\n",
    "\n",
    "In this first activity, you will implement a POMDP model in Python. You will start by loading the POMDP information from a `numpy` binary file, using the `numpy` function `load`. The file contains the list of states, actions, observations, transition probability matrices, observation probability matrices, and cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQ6IPaNBbjc0"
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 1.        \n",
    "\n",
    "Write a function named `load_pomdp` that receives, as input, a string corresponding to the name of the file with the POMDP information, and a real number $\\gamma$ between $0$ and $1$. The loaded file contains 6 arrays:\n",
    "\n",
    "* An array `X` that contains all the states in the POMDP, represented as strings. In the police-bandit scenario above, for example, there is a total of 25 states, each describing a stage in the game. The state is represented as x(y) where x is the location of the police and y the location of the bandit\n",
    "* An array `A` that contains all the actions in the POMDP, also represented as strings. In the  domain above, for example, each action is represented as one of the actions 'left', 'right', 'catch', or 'observe'.\n",
    "* An array `Z` that contains all the observations in the POMDP, also represented as strings. In the domain above, for example, there is a total of 6 observations.\n",
    "* An array `P` containing `len(A)` subarrays, each with dimension `len(X)` &times; `len(X)` and  corresponding to the transition probability matrix for one action.\n",
    "* An array `O` containing `len(A)` subarrays, each with dimension `len(X)` &times; `len(Z)` and  corresponding to the observation probability matrix for one action.\n",
    "* An array `c` with dimension `len(X)` &times; `len(A)` containing the cost function for the POMDP.\n",
    "\n",
    "Your function should create the POMDP as a tuple `(X, A, Z, (Pa, a = 0, ..., len(A)), (Oa, a = 0, ..., len(A)), c, g)`, where `X` is a tuple containing the states in the POMDP represented as strings (see above), `A` is a tuple containing the actions in the POMDP represented as strings (see above), `Z` is a tuple containing the observations in the POMDP represented as strings (see above), `P` is a tuple with `len(A)` elements, where `P[a]` is an `np.array` corresponding to the transition probability matrix for action `a`, `O` is a tuple with `len(A)` elements, where `O[a]` is an `np.array` corresponding to the observation probability matrix for action `a`, `c` is an `np.array` corresponding to the cost function for the POMDP, and `g` is a float, corresponding to the discount and provided as the argument $\\gamma$ of your function. Your function should return the POMDP tuple.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T14:09:53.429443Z",
     "start_time": "2022-04-03T14:09:53.210129Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OktbamhYbjc1",
    "outputId": "a04baf1a-fa52-46b2-ab94-417d030e61fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= State space (25 states) =\n",
      "\n",
      "States:\n",
      "0(0)\n",
      "0(1)\n",
      "0(2)\n",
      "0(3)\n",
      "0(4)\n",
      "1(0)\n",
      "1(1)\n",
      "1(2)\n",
      "1(3)\n",
      "1(4)\n",
      "2(0)\n",
      "2(1)\n",
      "2(2)\n",
      "2(3)\n",
      "2(4)\n",
      "3(0)\n",
      "3(1)\n",
      "3(2)\n",
      "3(3)\n",
      "3(4)\n",
      "4(0)\n",
      "4(1)\n",
      "4(2)\n",
      "4(3)\n",
      "4(4)\n",
      "\n",
      "Random state: x = 1(1)\n",
      "\n",
      "Last state: 4(4)\n",
      "= Action space (4 actions) =\n",
      "left\n",
      "right\n",
      "observ\n",
      "catch\n",
      "\n",
      "Random action: a = catch\n",
      "= Observation space (6 observations) =\n",
      "\n",
      "Observations:\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "e\n",
      "\n",
      "Random observation: z = 4\n",
      "\n",
      "Last observation: e\n",
      "\n",
      "= Transition probabilities =\n",
      "\n",
      "Transition probability matrix dimensions (action left): (25, 25)\n",
      "Dimensions add up for action \"left\"? True\n",
      "\n",
      "Transition probability matrix dimensions (action right): (25, 25)\n",
      "Dimensions add up for action \"right\"? True\n",
      "\n",
      "Transition probability matrix dimensions (action observ): (25, 25)\n",
      "Dimensions add up for action \"observ\"? True\n",
      "\n",
      "Transition probability matrix dimensions (action catch): (25, 25)\n",
      "Dimensions add up for action \"catch\"? True\n",
      "\n",
      "State-action pair (1(1), catch) transitions to state(s)\n",
      "s' in ['0(2)']\n",
      "\n",
      "= Observation probabilities =\n",
      "\n",
      "Observation probability matrix dimensions (action left): (25, 6)\n",
      "Dimensions add up for action \"left\"? True\n",
      "\n",
      "Observation probability matrix dimensions (action right): (25, 6)\n",
      "Dimensions add up for action \"right\"? True\n",
      "\n",
      "Observation probability matrix dimensions (action observ): (25, 6)\n",
      "Dimensions add up for action \"observ\"? True\n",
      "\n",
      "Observation probability matrix dimensions (action catch): (25, 6)\n",
      "Dimensions add up for action \"catch\"? True\n",
      "\n",
      "State-action pair (1(1), catch) yields observation(s)\n",
      "z in ['e']\n",
      "\n",
      "= Costs =\n",
      "\n",
      "Cost for the state-action pair (1(1), catch):\n",
      "c(s, a) = 0.0\n",
      "\n",
      "= Discount =\n",
      "\n",
      "gamma = 0.99\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_pomdp(fname, gamma):\n",
    "    \"\"\"\n",
    "    Builds an POMDP model from the provided file.\n",
    "\n",
    "    :param fname: Name of the file containing the POMDP information\n",
    "    :type: str\n",
    "    :param gamma: Discount\n",
    "    :type: float\n",
    "    :returns: tuple (tuple, tuple, tuple, tuple, tuple, nd.array, float)\n",
    "    \"\"\"\n",
    "\n",
    "    pomdp = np.load(fname)\n",
    "    X = tuple(pomdp[\"X\"])\n",
    "    A = tuple(pomdp[\"A\"])\n",
    "    Z = tuple(pomdp[\"Z\"])\n",
    "    P = tuple(pomdp[\"P\"])\n",
    "    O = tuple(pomdp[\"O\"])\n",
    "    c = pomdp[\"c\"]\n",
    "\n",
    "    return (X, A, Z, P, O, c, gamma)\n",
    "\n",
    "# -- End: load_pomdp\n",
    "\n",
    "import numpy.random as rand\n",
    "\n",
    "M = load_pomdp('linearcatch.npz', 0.99)\n",
    "\n",
    "rand.seed(42)\n",
    "\n",
    "# States\n",
    "print('= State space (%i states) =' % len(M[0]))\n",
    "print('\\nStates:')\n",
    "for i in range(len(M[0])):\n",
    "    print(M[0][i])\n",
    "\n",
    "# Random state\n",
    "x = rand.randint(len(M[0]))\n",
    "print('\\nRandom state: x =', M[0][x])\n",
    "\n",
    "# Last state\n",
    "print('\\nLast state:', M[0][-1])\n",
    "\n",
    "# Actions\n",
    "print('= Action space (%i actions) =' % len(M[1]))\n",
    "for i in range(len(M[1])):\n",
    "    print(M[1][i])\n",
    "\n",
    "# Random action\n",
    "a = rand.randint(len(M[1]))\n",
    "print('\\nRandom action: a =', M[1][a])\n",
    "\n",
    "# Observations\n",
    "print('= Observation space (%i observations) =' % len(M[2]))\n",
    "print('\\nObservations:')\n",
    "for i in range(len(M[2])):\n",
    "    print(M[2][i])\n",
    "\n",
    "# Random observation\n",
    "z = rand.randint(len(M[2]))\n",
    "print('\\nRandom observation: z =', M[2][z])\n",
    "\n",
    "# Last state\n",
    "print('\\nLast observation:', M[2][-1])\n",
    "\n",
    "# Transition probabilities\n",
    "print('\\n= Transition probabilities =')\n",
    "\n",
    "for i in range(len(M[1])):\n",
    "    print('\\nTransition probability matrix dimensions (action %s):' % M[1][i], M[3][i].shape)\n",
    "    print('Dimensions add up for action \"%s\"?' % M[1][i], np.isclose(np.sum(M[3][i]), len(M[0])))\n",
    "\n",
    "print('\\nState-action pair (%s, %s) transitions to state(s)' % (M[0][x], M[1][a]))\n",
    "print(\"s' in\", np.array(M[0])[np.where(M[3][a][x, :] > 0)])\n",
    "\n",
    "# Observation probabilities\n",
    "print('\\n= Observation probabilities =')\n",
    "\n",
    "for i in range(len(M[1])):\n",
    "    print('\\nObservation probability matrix dimensions (action %s):' % M[1][i], M[4][i].shape)\n",
    "    print('Dimensions add up for action \"%s\"?' % M[1][i], np.isclose(np.sum(M[4][i]), len(M[0])))\n",
    "\n",
    "print('\\nState-action pair (%s, %s) yields observation(s)' % (M[0][x], M[1][a]))\n",
    "print(\"z in\", np.array(M[2])[np.where(M[4][a][x, :] > 0)])\n",
    "\n",
    "# Cost\n",
    "print('\\n= Costs =')\n",
    "\n",
    "print('\\nCost for the state-action pair (%s, %s):' % (M[0][x], M[1][a]))\n",
    "print('c(s, a) =', M[5][x, a])\n",
    "\n",
    "# Discount\n",
    "print('\\n= Discount =')\n",
    "print('\\ngamma =', M[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ds2wVHuvbjc2"
   },
   "source": [
    "We provide below an example of application of the function with the file `pomdp.npz` that you can use as a first \"sanity check\" for your code. Note that, even fixing the seed, the results you obtain may slightly differ.\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "= State space (25 states) =\n",
    "\n",
    "States:\n",
    "0(0)\n",
    "0(1)\n",
    "0(2)\n",
    "0(3)\n",
    "0(4)\n",
    "1(0)\n",
    "1(1)\n",
    "1(2)\n",
    "1(3)\n",
    "1(4)\n",
    "2(0)\n",
    "2(1)\n",
    "2(2)\n",
    "2(3)\n",
    "2(4)\n",
    "3(0)\n",
    "3(1)\n",
    "3(2)\n",
    "3(3)\n",
    "3(4)\n",
    "4(0)\n",
    "4(1)\n",
    "4(2)\n",
    "4(3)\n",
    "4(4)\n",
    "\n",
    "Random state: x = 1(1)\n",
    "\n",
    "Last state: 4(4)\n",
    "= Action space (4 actions) =\n",
    "left\n",
    "right\n",
    "observ\n",
    "catch\n",
    "\n",
    "Random action: a = catch\n",
    "= Observation space (6 observations) =\n",
    "\n",
    "Observations:\n",
    "0\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "e\n",
    "\n",
    "Random observation: z = 4\n",
    "\n",
    "Last observation: e\n",
    "\n",
    "= Transition probabilities =\n",
    "\n",
    "Transition probability matrix dimensions (action left): (25, 25)\n",
    "Dimensions add up for action \"left\"? True\n",
    "\n",
    "Transition probability matrix dimensions (action right): (25, 25)\n",
    "Dimensions add up for action \"right\"? True\n",
    "\n",
    "Transition probability matrix dimensions (action observ): (25, 25)\n",
    "Dimensions add up for action \"observ\"? True\n",
    "\n",
    "Transition probability matrix dimensions (action catch): (25, 25)\n",
    "Dimensions add up for action \"catch\"? True\n",
    "\n",
    "State-action pair (1(1), catch) transitions to state(s)\n",
    "s' in ['0(2)']\n",
    "\n",
    "= Observation probabilities =\n",
    "\n",
    "Observation probability matrix dimensions (action left): (25, 6)\n",
    "Dimensions add up for action \"left\"? True\n",
    "\n",
    "Observation probability matrix dimensions (action right): (25, 6)\n",
    "Dimensions add up for action \"right\"? True\n",
    "\n",
    "Observation probability matrix dimensions (action observ): (25, 6)\n",
    "Dimensions add up for action \"observ\"? True\n",
    "\n",
    "Observation probability matrix dimensions (action catch): (25, 6)\n",
    "Dimensions add up for action \"catch\"? True\n",
    "\n",
    "State-action pair (1(1), catch) yields observation(s)\n",
    "z in ['e']\n",
    "\n",
    "= Costs =\n",
    "\n",
    "Cost for the state-action pair (1(1), catch):\n",
    "c(s, a) = 0.0\n",
    "\n",
    "= Discount =\n",
    "\n",
    "gamma = 0.99\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lL-0HpDXbjc3"
   },
   "source": [
    "### 2. Sampling\n",
    "\n",
    "You are now going to sample random trajectories of your POMDP and observe the impact it has on the corresponding belief."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeYWCJXBbjc3",
    "raw_mimetype": "text/latex"
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 2.\n",
    "\n",
    "Write a function called `gen_trajectory` that generates a random POMDP trajectory using a uniformly random policy. Your function should receive, as input, a POMDP described as a tuple like that from **Activity 1** and two integers, `x0` and `n` and return a tuple with 3 elements, where:\n",
    "\n",
    "1. The first element is a `numpy` array corresponding to a sequence of `n + 1` state indices, $x_0,x_1,\\ldots,x_n$, visited by the agent when following a uniform policy (i.e., a policy where actions are selected uniformly at random) from state with index `x0`. In other words, you should select $x_1$ from $x_0$ using a random action; then $x_2$ from $x_1$, etc.\n",
    "2. The second element is a `numpy` array corresponding to the sequence of `n` action indices, $a_0,\\ldots,a_{n-1}$, used in the generation of the trajectory in 1.;\n",
    "3. The third element is a `numpy` array corresponding to the sequence of `n` observation indices, $z_1,\\ldots,z_n$, experienced by the agent during the trajectory in 1.\n",
    "\n",
    "The `numpy` array in 1. should have a shape `(n + 1,)`; the `numpy` arrays from 2. and 3. should have a shape `(n,)`.\n",
    "\n",
    "**Note:** Your function should work for **any** POMDP specified as above.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T14:09:53.451420Z",
     "start_time": "2022-04-03T14:09:53.431531Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_oIhG52mbjc3",
    "outputId": "f5699dda-bf9d-48ea-8560-8896ec94b47f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of state trajectory: (11,)\n",
      "Shape of state trajectory: (10,)\n",
      "Shape of state trajectory: (10,)\n",
      "\n",
      "- Time step 0 -\n",
      "State: 0(0) (state 0)\n",
      "Action selected: observ (action 2)\n",
      "Resulting state: 0(0) (state 0)\n",
      "Observation: 0 (observation 0)\n",
      "\n",
      "- Time step 1 -\n",
      "State: 0(0) (state 0)\n",
      "Action selected: catch (action 3)\n",
      "Resulting state: 0(2) (state 2)\n",
      "Observation: e (observation 5)\n",
      "\n",
      "- Time step 2 -\n",
      "State: 0(2) (state 2)\n",
      "Action selected: observ (action 2)\n",
      "Resulting state: 0(2) (state 2)\n",
      "Observation: 2 (observation 2)\n",
      "\n",
      "- Time step 3 -\n",
      "State: 0(2) (state 2)\n",
      "Action selected: left (action 0)\n",
      "Resulting state: 4(2) (state 22)\n",
      "Observation: e (observation 5)\n",
      "\n",
      "- Time step 4 -\n",
      "State: 4(2) (state 22)\n",
      "Action selected: right (action 1)\n",
      "Resulting state: 0(1) (state 1)\n",
      "Observation: e (observation 5)\n",
      "\n",
      "- Time step 5 -\n",
      "State: 0(1) (state 1)\n",
      "Action selected: right (action 1)\n",
      "Resulting state: 1(1) (state 6)\n",
      "Observation: e (observation 5)\n",
      "\n",
      "- Time step 6 -\n",
      "State: 1(1) (state 6)\n",
      "Action selected: left (action 0)\n",
      "Resulting state: 0(1) (state 1)\n",
      "Observation: e (observation 5)\n",
      "\n",
      "- Time step 7 -\n",
      "State: 0(1) (state 1)\n",
      "Action selected: left (action 0)\n",
      "Resulting state: 4(1) (state 21)\n",
      "Observation: e (observation 5)\n",
      "\n",
      "- Time step 8 -\n",
      "State: 4(1) (state 21)\n",
      "Action selected: observ (action 2)\n",
      "Resulting state: 4(1) (state 21)\n",
      "Observation: 1 (observation 1)\n",
      "\n",
      "- Time step 9 -\n",
      "State: 4(1) (state 21)\n",
      "Action selected: catch (action 3)\n",
      "Resulting state: 4(1) (state 21)\n",
      "Observation: e (observation 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy.random as rnd\n",
    "\n",
    "def gen_trajectory(pomdp, x0, steps):\n",
    "    \"\"\"\n",
    "    Generates a random trajectory for the provided POMDP from the provided initial state and with\n",
    "    the provided number of steps.\n",
    "\n",
    "    :param pomdp: POMDP description\n",
    "    :type: tuple\n",
    "    :param x0: Initial state\n",
    "    :type: int\n",
    "    :param steps: Number of steps\n",
    "    :type: int\n",
    "    :returns: tuple (nd.array, nd.array, nd.array)\n",
    "    \"\"\"\n",
    "\n",
    "    X, A, Z, P, O, c, gamma = pomdp\n",
    "\n",
    "    state_trajectory = np.zeros(steps + 1, dtype=int)\n",
    "    action_trajectory = np.zeros(steps, dtype=int)\n",
    "    observation_trajectory = np.zeros(steps, dtype=int)\n",
    "\n",
    "    state_trajectory[0] = x0\n",
    "\n",
    "    for i in range(steps):\n",
    "        #escolher random açao\n",
    "        a = rnd.randint(len(A))\n",
    "        action_trajectory[i] = a\n",
    "\n",
    "        #vai para o proximo estado\n",
    "        next_state = rnd.choice(len(X), p=P[a][state_trajectory[i]])\n",
    "        state_trajectory[i + 1] = next_state\n",
    "\n",
    "        #gera observacao\n",
    "        observation = rnd.choice(len(Z), p=O[a][next_state])\n",
    "        observation_trajectory[i] = observation\n",
    "\n",
    "    return (state_trajectory, action_trajectory, observation_trajectory)\n",
    "\n",
    "# -- End: gen_trajectory\n",
    "\n",
    "rand.seed(42)\n",
    "\n",
    "# Number of steps and initial state\n",
    "steps = 10\n",
    "x0    = 0 # State I\n",
    "\n",
    "# Generate trajectory\n",
    "t = gen_trajectory(M, x0, steps)\n",
    "\n",
    "# Check shapes\n",
    "print('Shape of state trajectory:', t[0].shape)\n",
    "print('Shape of state trajectory:', t[1].shape)\n",
    "print('Shape of state trajectory:', t[2].shape)\n",
    "\n",
    "# Print trajectory\n",
    "for i in range(steps):\n",
    "    print('\\n- Time step %i -' % i)\n",
    "    print('State:', M[0][t[0][i]], '(state %i)' % t[0][i])\n",
    "    print('Action selected:', M[1][t[1][i]], '(action %i)' % t[1][i])\n",
    "    print('Resulting state:', M[0][t[0][i+1]], '(state %i)' % t[0][i+1])\n",
    "    print('Observation:', M[2][t[2][i]], '(observation %i)' % t[2][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMm1mHJXbjc4"
   },
   "source": [
    "For example, using the POMDP from **Activity 1** you could obtain the following interaction.\n",
    "\n",
    "```python\n",
    "rand.seed(42)\n",
    "\n",
    "# Number of steps and initial state\n",
    "steps = 10\n",
    "x0    = 0 # State I\n",
    "\n",
    "# Generate trajectory\n",
    "t = gen_trajectory(M, x0, steps)\n",
    "\n",
    "# Check shapes\n",
    "print('Shape of state trajectory:', t[0].shape)\n",
    "print('Shape of state trajectory:', t[1].shape)\n",
    "print('Shape of state trajectory:', t[2].shape)\n",
    "\n",
    "# Print trajectory\n",
    "for i in range(steps):\n",
    "    print('\\n- Time step %i -' % i)\n",
    "    print('State:', M[0][t[0][i]], '(state %i)' % t[0][i])\n",
    "    print('Action selected:', M[1][t[1][i]], '(action %i)' % t[1][i])\n",
    "    print('Resulting state:', M[0][t[0][i+1]], '(state %i)' % t[0][i+1])\n",
    "    print('Observation:', M[2][t[2][i]], '(observation %i)' % t[2][i])\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Shape of state trajectory: (11,)\n",
    "Shape of state trajectory: (10,)\n",
    "Shape of state trajectory: (10,)\n",
    "\n",
    "- Time step 0 -\n",
    "State: 0(0) (state 0)\n",
    "Action selected: observ (action 2)\n",
    "Resulting state: 0(0) (state 0)\n",
    "Observation: 0 (observation 0)\n",
    "\n",
    "- Time step 1 -\n",
    "State: 0(0) (state 0)\n",
    "Action selected: catch (action 3)\n",
    "Resulting state: 0(2) (state 2)\n",
    "Observation: e (observation 5)\n",
    "\n",
    "- Time step 2 -\n",
    "State: 0(2) (state 2)\n",
    "Action selected: observ (action 2)\n",
    "Resulting state: 0(2) (state 2)\n",
    "Observation: 2 (observation 2)\n",
    "\n",
    "- Time step 3 -\n",
    "State: 0(2) (state 2)\n",
    "Action selected: left (action 0)\n",
    "Resulting state: 4(2) (state 22)\n",
    "Observation: e (observation 5)\n",
    "\n",
    "- Time step 4 -\n",
    "State: 4(2) (state 22)\n",
    "Action selected: right (action 1)\n",
    "Resulting state: 0(1) (state 1)\n",
    "Observation: e (observation 5)\n",
    "\n",
    "- Time step 5 -\n",
    "State: 0(1) (state 1)\n",
    "Action selected: right (action 1)\n",
    "Resulting state: 1(1) (state 6)\n",
    "Observation: e (observation 5)\n",
    "\n",
    "- Time step 6 -\n",
    "State: 1(1) (state 6)\n",
    "Action selected: left (action 0)\n",
    "Resulting state: 0(1) (state 1)\n",
    "Observation: e (observation 5)\n",
    "\n",
    "- Time step 7 -\n",
    "State: 0(1) (state 1)\n",
    "Action selected: left (action 0)\n",
    "Resulting state: 4(1) (state 21)\n",
    "Observation: e (observation 5)\n",
    "\n",
    "- Time step 8 -\n",
    "State: 4(1) (state 21)\n",
    "Action selected: observ (action 2)\n",
    "Resulting state: 4(1) (state 21)\n",
    "Observation: 1 (observation 1)\n",
    "\n",
    "- Time step 9 -\n",
    "State: 4(1) (state 21)\n",
    "Action selected: catch (action 3)\n",
    "Resulting state: 4(1) (state 21)\n",
    "Observation: e (observation 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4iBaS69bjc4"
   },
   "source": [
    "You will now write a function that samples a given number of possible belief points for a POMDP. To do that, you will use the function from **Activity 2**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Activity 3.\n",
    "\n",
    "Write a function called `sample_beliefs` that receives, as input, a POMDP described as a tuple like that from **Activity 1** and an integer `n`, and return a tuple with `n + 1` elements **or less**, each corresponding to a possible belief state (represented as a $1\\times|\\mathcal{X}|$ vector). To do so, your function should\n",
    "\n",
    "* Generate a trajectory with `n` steps from a random initial state, using the function `gen_trajectory` from **Activity 2**.\n",
    "* For the generated trajectory, compute the corresponding sequence of beliefs, assuming that the agent does not know its initial state (i.e., the initial belief is the uniform belief, and should also be considered).\n",
    "\n",
    "Your function should return a tuple with the resulting beliefs, **ignoring duplicate beliefs or beliefs whose distance is smaller than $10^{-3}$.**\n",
    "\n",
    "**Suggestion:** You may want to define an auxiliary function `belief_update` that receives a POMDP, a belief, an action and an observation and returns the updated belief.\n",
    "\n",
    "**Note:** Your function should work for **any** POMDP specified as above. To compute the distance between vectors, you may find useful `numpy`'s function `linalg.norm`.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T14:09:53.536357Z",
     "start_time": "2022-04-03T14:09:53.455872Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pvduCH58bjc4",
    "outputId": "948cbf9b-ad28-4edb-c9a4-3e7f6f791a59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 25)\n",
      "4 beliefs sampled:\n",
      "[[0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04\n",
      "  0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04]]\n",
      "Belief adds to 1? True\n",
      "[[0.012 0.032 0.24  0.04  0.036 0.036 0.012 0.032 0.04  0.04  0.04  0.036\n",
      "  0.012 0.032 0.04  0.04  0.04  0.036 0.012 0.032 0.032 0.04  0.04  0.036\n",
      "  0.012]]\n",
      "Belief adds to 1? True\n",
      "[[0.034 0.019 0.029 0.038 0.04  0.04  0.034 0.019 0.029 0.038 0.038 0.04\n",
      "  0.034 0.019 0.029 0.029 0.038 0.04  0.034 0.019 0.019 0.049 0.178 0.08\n",
      "  0.034]]\n",
      "Belief adds to 1? True\n",
      "[[0.    0.    0.096 0.    0.    0.    0.    0.063 0.    0.    0.    0.\n",
      "  0.115 0.    0.    0.    0.    0.132 0.    0.    0.    0.    0.595 0.\n",
      "  0.   ]]\n",
      "Belief adds to 1? True\n",
      "(1, 25)\n",
      "94 beliefs sampled.\n"
     ]
    }
   ],
   "source": [
    "def belief_update(pomdp, belief, action, obs):\n",
    "    Pa = pomdp[3][action]\n",
    "    Oa = np.diag(pomdp[4][action][:, obs]) # Obs da acao\n",
    "    \n",
    "    # belief_t+1 = b_t * Pa * diag(Oa(z | .)) / || b_t * Pa * diag(Oa(z | .)) ||\n",
    "    unNormBelief = belief @ Pa @ Oa\n",
    "    updatedBelief =  unNormBelief / np.linalg.norm(unNormBelief, ord=1)\n",
    "\n",
    "    return updatedBelief\n",
    "\n",
    "def computeDistance(beliefs, contender):\n",
    "\n",
    "    for idx, belief in enumerate(beliefs):\n",
    "        d = np.linalg.norm(contender - belief)\n",
    "\n",
    "        if d < 1e-3:\n",
    "            return -1\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def sample_beliefs(mdp, n):\n",
    "    \"\"\"\n",
    "    Generates a random sample of belief states for the provided POMDP.\n",
    "\n",
    "    :param mdp: POMDP description\n",
    "    :type: tuple\n",
    "    :param steps: Maximum number of sampled beliefs\n",
    "    :type: int\n",
    "    :returns: tuple (n x nd.array)\n",
    "    \"\"\"\n",
    "    b0 = np.ones(len(mdp[0])) / len(mdp[0])\n",
    "    x0 = np.random.randint(len(mdp[0]))\n",
    "\n",
    "    trajectory = gen_trajectory(mdp, x0, n)\n",
    "    b0 = b0.reshape(1, -1)\n",
    "    beliefs = [b0]\n",
    "    print(beliefs[-1].shape)\n",
    "    for i in range(n):\n",
    "        newBelief = belief_update(mdp, beliefs[-1][-1], trajectory[1][i], trajectory[2][i]).reshape(1, -1)\n",
    "        if computeDistance(beliefs=beliefs, contender=newBelief) == 0:\n",
    "            beliefs.append(newBelief)\n",
    "        \n",
    "    return tuple(beliefs)\n",
    "\n",
    "# -- sample_belief\n",
    "\n",
    "rand.seed(42)\n",
    "\n",
    "# 3 sample beliefs + initial belief\n",
    "B = sample_beliefs(M, 3)\n",
    "print('%i beliefs sampled:' % len(B))\n",
    "for i in range(len(B)):\n",
    "    print(np.round(B[i], 3))\n",
    "    print('Belief adds to 1?', np.isclose(B[i].sum(), 1.))\n",
    "\n",
    "# 100 sample beliefs\n",
    "B = sample_beliefs(M, 100)\n",
    "print('%i beliefs sampled.' % len(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZjkMdtcbjc5"
   },
   "source": [
    "For example, using the POMDP from **Activity 1** you could obtain the following interaction.\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "\n",
    "4 beliefs sampled:\n",
    "[[0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04\n",
    "  0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04]]\n",
    "Belief adds to 1? True\n",
    "[[0.012 0.032 0.24  0.04  0.036 0.036 0.012 0.032 0.04  0.04  0.04  0.036\n",
    "  0.012 0.032 0.04  0.04  0.04  0.036 0.012 0.032 0.032 0.04  0.04  0.036\n",
    "  0.012]]\n",
    "Belief adds to 1? True\n",
    "[[0.034 0.019 0.029 0.038 0.04  0.04  0.034 0.019 0.029 0.038 0.038 0.04\n",
    "  0.034 0.019 0.029 0.029 0.038 0.04  0.034 0.019 0.019 0.049 0.178 0.08\n",
    "  0.034]]\n",
    "Belief adds to 1? True\n",
    "[[0.    0.    0.096 0.    0.    0.    0.    0.063 0.    0.    0.    0.\n",
    "  0.115 0.    0.    0.    0.    0.132 0.    0.    0.    0.    0.595 0.\n",
    "  0.   ]]\n",
    "Belief adds to 1? True\n",
    "94 beliefs sampled.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5nekOCLTSCD"
   },
   "source": [
    "<font color=\"cyan\">**Question 1**: Assume the initial belief is, as implemented above, the uniform belief. **Q1.1** If we were to consider a different policy than the random policy to sample beliefs, should we expect to get a different set of sampled beliefs? **Q1.2** Is our code above able to retrieve all possible beliefs that can be attained under all policies (assume we are able to sample an infinite number of trajectories of infinite-length)? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJilHbMU1nCN"
   },
   "source": [
    "<font color=\"cyan\">\n",
    "\n",
    "**Insert answer:** **Q1.1** \n",
    "\n",
    "In fact, if we replace the random policy with a new one, we should anticipate a different set of sampled beliefs. The belief updates are determined by the sequence of activities taken and the related observations made. A different distribution across actions would result from a different policy, which would in turn result in a different set of observations and, thus, different updates to beliefs.\n",
    "\n",
    "**Insert answer:** **Q1.2** \n",
    "\n",
    "No, the provided code does not guarantee that it will obtain every belief that could be reached under every policy, even if we sample an infinite number of infinite-length trajectories. Because the algorithm samples beliefs based on a policy, which in this case is random, it only looks at the belief space that could be reached under that policy. Certain regions may remain unreachable if the policy does not adequately explore them, even though the belief space is spanned by an infinite number of trajectories. A more comprehensive method would need actively ensuring comprehensive research of the belief space, perhaps by means of concentrated exploration strategies.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6Vao9Lpbjc5"
   },
   "source": [
    "### 3. MDP-based heuristics\n",
    "\n",
    "In this section you are going to compare different heuristic approaches for POMDPs discussed in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQitbM_mbjc5"
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 4\n",
    "\n",
    "Write a function `solve_mdp` that takes as input a POMDP represented as a tuple like that of **Activity 1** and returns a `numpy` array corresponding to the **optimal $Q$-function for the underlying MDP**. Stop the algorithm when the error between iterations is smaller than $10^{-8}$.\n",
    "\n",
    "**Note:** Your function should work for **any** POMDP specified as above. Feel free to reuse one of the functions you implemented in Lab 2 (for example, value iteration).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T14:09:53.575520Z",
     "start_time": "2022-04-03T14:09:53.538939Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lcvLPTj2bjc5",
    "outputId": "614931e3-ef35-42f0-f29e-09a452d53325"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q-values at state 1(1): [71.743 71.664 71.315 71.025]\n",
      "Best action at state 1(1): catch\n",
      "\n",
      "Q-values at state 0(3): [71.664 71.979 71.947 71.91 ]\n",
      "Best action at state 0(3): left\n",
      "\n",
      "Q-values at state 2(2): [71.743 71.664 71.315 71.025]\n",
      "Best action at state 2(2): catch\n"
     ]
    }
   ],
   "source": [
    "def solve_mdp(pomdp):\n",
    "    \"\"\"\n",
    "    Computes the optimal Q-function for the underlying MDP.\n",
    "\n",
    "    :param pomdp: POMDP description\n",
    "    :type: tuple\n",
    "    :returns: nd.array\n",
    "    \"\"\"\n",
    "\n",
    "    X, A, Z, P, O, c, gamma = pomdp\n",
    "\n",
    "    num_states = len(X)\n",
    "    num_actions = len(A)\n",
    "\n",
    "    #Copiado do lab anterior (value iteration)\n",
    "    J = np.zeros(num_states)\n",
    "\n",
    "    epsilon = 1e-8\n",
    "    error = float('inf')\n",
    "\n",
    "    #Value iteration loop\n",
    "    while error >= epsilon:\n",
    "        J_prev = J.copy()\n",
    "        for x in range(num_states):\n",
    "            J[x] = min([c[x, a] + gamma * np.sum(P[a][x, :] * J_prev) for a in range(num_actions)])\n",
    "        error = np.linalg.norm(J - J_prev)\n",
    "\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    for x in range(num_states):\n",
    "        for a in range(num_actions):\n",
    "            Q[x, a] = c[x, a] + gamma * np.sum(P[a][x, :] * J)\n",
    "\n",
    "    return Q\n",
    "\n",
    "# -- End: solve_mdp\n",
    "\n",
    "Q = solve_mdp(M)\n",
    "\n",
    "x = 6 # State C\n",
    "print('\\nQ-values at state %s:' % M[0][x], np.round(Q[x, :], 3))\n",
    "print('Best action at state %s:' % M[0][x], M[1][np.argmin(Q[x, :])])\n",
    "\n",
    "x = 3 # State 2C\n",
    "print('\\nQ-values at state %s:' % M[0][x], np.round(Q[x, :], 3))\n",
    "print('Best action at state %s:' % M[0][x], M[1][np.argmin(Q[x, :])])\n",
    "\n",
    "x = 12 # L\n",
    "print('\\nQ-values at state %s:' % M[0][x], np.round(Q[x, :], 3))\n",
    "print('Best action at state %s:' % M[0][x], M[1][np.argmin(Q[x, :])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhEtMAqqbjc5"
   },
   "source": [
    "As an example, you can run the following code on the POMDP from **Activity 1**.\n",
    "\n",
    "```python\n",
    "Q = solve_mdp(M)\n",
    "\n",
    "x = 6 # State C\n",
    "print('\\nQ-values at state %s:' % M[0][x], np.round(Q[x, :], 3))\n",
    "print('Best action at state %s:' % M[0][x], M[1][np.argmin(Q[x, :])])\n",
    "\n",
    "x = 3 # State 2C\n",
    "print('\\nQ-values at state %s:' % M[0][x], np.round(Q[x, :], 3))\n",
    "print('Best action at state %s:' % M[0][x], M[1][np.argmin(Q[x, :])])\n",
    "\n",
    "x = 12 # L\n",
    "print('\\nQ-values at state %s:' % M[0][x], np.round(Q[x, :], 3))\n",
    "print('Best action at state %s:' % M[0][x], M[1][np.argmin(Q[x, :])])\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Q-values at state 1(1): [71.743 71.664 71.315 71.025]\n",
    "Best action at state 1(1): catch\n",
    "\n",
    "Q-values at state 0(3): [71.664 71.979 71.947 71.91 ]\n",
    "Best action at state 0(3): left\n",
    "\n",
    "Q-values at state 2(2): [71.743 71.664 71.315 71.025]\n",
    "Best action at state 2(2): catch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJXEAM7zbjc6"
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 5\n",
    "\n",
    "You will now test the different MDP heuristics discussed in class. To that purpose, write down a function that, given a belief vector and the solution for the underlying MDP, computes the action prescribed by each of the three MDP heuristics. In particular, you should write down a function named `get_heuristic_action` that receives, as inputs:\n",
    "\n",
    "* A belief state represented as a `numpy` array like those of **Activity 3**;\n",
    "* The optimal $Q$-function for an MDP (computed, for example, using the function `solve_mdp` from **Activity 4**);\n",
    "* A string that can be either `\"mls\"`, `\"av\"`, or `\"q-mdp\"`;\n",
    "\n",
    "Your function should return an integer corresponding to the index of the action prescribed by the heuristic indicated by the corresponding string, i.e., the most likely state heuristic for `\"mls\"`, the action voting heuristic for `\"av\"`, and the $Q$-MDP heuristic for `\"q-mdp\"`. *In all heuristics, ties should be broken randomly, i.e., when maximizing/minimizing, you should randomly select between all maximizers/minimizers*.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T14:09:53.631572Z",
     "start_time": "2022-04-03T14:09:53.578340Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AbpxQnQTbjc6",
    "outputId": "0801f945-29c9-403e-d236-18c9dcc4a681"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Belief (approx.) uniform\n",
      "MLS action: catch; AV action: left; Q-MDP action: catch\n",
      "\n",
      "Belief (approx.) uniform\n",
      "MLS action: right; AV action: right; Q-MDP action: right\n",
      "\n",
      "Belief (approx.) uniform\n",
      "MLS action: right; AV action: right; Q-MDP action: right\n",
      "\n",
      "Belief (approx.) uniform\n",
      "MLS action: catch; AV action: right; Q-MDP action: catch\n",
      "\n",
      "Belief (approx.) uniform\n",
      "MLS action: right; AV action: right; Q-MDP action: right\n",
      "\n",
      "Belief (approx.) uniform\n",
      "MLS action: left; AV action: left; Q-MDP action: left\n",
      "\n",
      "Belief (approx.) uniform\n",
      "MLS action: right; AV action: right; Q-MDP action: right\n",
      "\n",
      "Belief: [ 0(2) : 0.625, 1(2) : 0.084, 2(2) : 0.119, 3(2) : 0.078, 4(2) : 0.094]\n",
      "MLS action: right; AV action: right; Q-MDP action: right\n",
      "\n",
      "Belief: [ 0(1) : 0.008, 0(2) : 0.059, 0(3) : 0.017, 1(1) : 0.012, 1(2) : 0.083, 1(3) : 0.024, 2(1) : 0.008, 2(2) : 0.054, 2(3) : 0.016, 3(1) : 0.009, 3(2) : 0.066, 3(3) : 0.019, 4(1) : 0.063, 4(2) : 0.438, 4(3) : 0.125]\n",
      "MLS action: left; AV action: left; Q-MDP action: left\n",
      "\n",
      "Belief (approx.) uniform\n",
      "MLS action: left; AV action: left; Q-MDP action: catch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_heuristic_action(belief, qfunction, heuristic):\n",
    "    \"\"\"\n",
    "    Computes the action prescribed by the selected MDP heuristic at the provided belief.\n",
    "\n",
    "    :param belief: belief vector\n",
    "    :type: nd.array\n",
    "    :param qfunction: optimal q-function for the underlying MDP\n",
    "    :type: nd.array\n",
    "    :param heuristic: selected heuristic\n",
    "    :type: str\n",
    "    :returns: int\n",
    "    \"\"\"\n",
    "\n",
    "    tol = 1e-6\n",
    "\n",
    "    belief = belief.flatten()\n",
    "    \n",
    "    if heuristic == 'q-mdp':\n",
    "        # E_b[Q(s,a)] = sum_s b(s)*Q(s,a)\n",
    "        exp_q = np.dot(belief, qfunction)  # shape (|A|,)\n",
    "        min_val = np.min(exp_q)\n",
    "        # Identify tied actions (within tol of the minimum)\n",
    "        tie_actions = np.where(np.abs(exp_q - min_val) < tol)[0]\n",
    "        return np.random.choice(tie_actions)\n",
    "    \n",
    "    elif heuristic == 'av':\n",
    "        num_actions = qfunction.shape[1]\n",
    "        votes = np.zeros(num_actions)\n",
    "        for s, prob in enumerate(belief):\n",
    "            optimal_action = np.argmin(qfunction[s, :])\n",
    "            votes[optimal_action] += prob\n",
    "        max_votes = np.max(votes)\n",
    "        tie_actions = np.where(np.abs(votes - max_votes) < tol)[0]\n",
    "        return np.random.choice(tie_actions)\n",
    "    \n",
    "    elif heuristic == 'mls':\n",
    "        most_likely_state = np.argmax(belief)\n",
    "        state_q = qfunction[most_likely_state, :]\n",
    "        min_val = np.min(state_q)\n",
    "        tie_actions = np.where(np.abs(state_q - min_val) < tol)[0]\n",
    "        return np.random.choice(tie_actions)\n",
    "\n",
    "\n",
    "# -- End: get_heuristic_action\n",
    "\n",
    "rand.seed(42)\n",
    "\n",
    "for b in B[:10]:\n",
    "\n",
    "    if np.all(b > 0):\n",
    "        print('Belief (approx.) uniform')\n",
    "    else:\n",
    "        initial = True\n",
    "\n",
    "        for i in range(len(M[0])):\n",
    "            if b[0, i] > 0:\n",
    "                if initial:\n",
    "                    initial = False\n",
    "                    print('Belief: [', M[0][i], ': %.3f' % np.round(b[0, i], 3), end='')\n",
    "                else:\n",
    "                    print(',', M[0][i], ': %.3f' % np.round(b[0, i], 3), end='')\n",
    "        print(']')\n",
    "\n",
    "    print('MLS action:', M[1][get_heuristic_action(b, Q, 'mls')], end='; ')\n",
    "    print('AV action:', M[1][get_heuristic_action(b, Q, 'av')], end='; ')\n",
    "    print('Q-MDP action:', M[1][get_heuristic_action(b, Q, 'q-mdp')])\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crOPkO1objc6"
   },
   "source": [
    "For example, if you run your function in the examples from **Activity 3** using the $Q$-function from **Activity 4**, you can observe the following interaction.\n",
    "\n",
    "```python\n",
    "rand.seed(42)\n",
    "\n",
    "for b in B[:10]:\n",
    "    \n",
    "    if np.all(b > 0):\n",
    "        print('Belief (approx.) uniform')\n",
    "    else:\n",
    "        initial = True\n",
    "\n",
    "        for i in range(len(M[0])):\n",
    "            if b[0, i] > 0:\n",
    "                if initial:\n",
    "                    initial = False\n",
    "                    print('Belief: [', M[0][i], ': %.3f' % np.round(b[0, i], 3), end='')\n",
    "                else:\n",
    "                    print(',', M[0][i], ': %.3f' % np.round(b[0, i], 3), end='')\n",
    "        print(']')\n",
    "\n",
    "    print('MLS action:', M[1][get_heuristic_action(b, Q, 'mls')], end='; ')\n",
    "    print('AV action:', M[1][get_heuristic_action(b, Q, 'av')], end='; ')\n",
    "    print('Q-MDP action:', M[1][get_heuristic_action(b, Q, 'q-mdp')])\n",
    "\n",
    "    print()\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Belief (approx.) uniform\n",
    "MLS action: left; AV action: right; Q-MDP action: catch\n",
    "\n",
    "Belief (approx.) uniform\n",
    "MLS action: right; AV action: right; Q-MDP action: right\n",
    "\n",
    "Belief (approx.) uniform\n",
    "MLS action: right; AV action: right; Q-MDP action: right\n",
    "\n",
    "Belief (approx.) uniform\n",
    "MLS action: catch; AV action: right; Q-MDP action: catch\n",
    "\n",
    "Belief (approx.) uniform\n",
    "MLS action: right; AV action: right; Q-MDP action: right\n",
    "\n",
    "Belief (approx.) uniform\n",
    "MLS action: left; AV action: left; Q-MDP action: left\n",
    "\n",
    "Belief (approx.) uniform\n",
    "MLS action: right; AV action: right; Q-MDP action: right\n",
    "\n",
    "Belief: [ 0(2) : 0.625, 1(2) : 0.084, 2(2) : 0.119, 3(2) : 0.078, 4(2) : 0.094]\n",
    "MLS action: right; AV action: right; Q-MDP action: right\n",
    "\n",
    "Belief: [ 0(1) : 0.008, 0(2) : 0.059, 0(3) : 0.017, 1(1) : 0.012, 1(2) : 0.083, 1(3) : 0.024, 2(1) : 0.008, 2(2) : 0.054, 2(3) : 0.016, 3(1) : 0.009, 3(2) : 0.066, 3(3) : 0.019, 4(1) : 0.063, 4(2) : 0.438, 4(3) : 0.125]\n",
    "MLS action: left; AV action: left; Q-MDP action: left\n",
    "\n",
    "Belief (approx.) uniform\n",
    "MLS action: left; AV action: left; Q-MDP action: catch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tiEJzfY2IFH"
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 6\n",
    "\n",
    "You will now test the different heuristics you implemented in the previous question. To do this, you will implement function `test_heuristic`, which receives as arguments:\n",
    "* A tuple specifying the POMDP;\n",
    "* The optimal $Q$-function for an MDP (computed, for example, using the function `solve_mdp` from **Activity 4**);\n",
    "* A string specifying the action-selection heuristic that can be either `\"mls\"`, `\"av\"`, or `\"q-mdp\"`;\n",
    "* An integer `n` specifying the legnth of each sampled trajectory;\n",
    "* An integer `NRUNS` specifying the number of sampled trajectories.\n",
    "\n",
    "The function should first randomly sample an initial state (with equal probability of sampling any state) and consider the initial belief is uniform over the state space; then, the function should let the agent iteratively interact with the environment while updating its belief and choosing actions according to the specified heuristic. The function should sample `NRUNS` trajectories, compute the discounted cumulative costs for each trajectory, and return the mean discounted cumulative costs obtained over the `NRUNS` sampled trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PKJHsOby96kc",
    "outputId": "b6efc772-7ecc-47c5-eb56-c538566ece24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_state: 2\n",
      "MLS heuristic: 35.47344484222369\n",
      "AV heuristic: 39.499393286246324\n",
      "Q-MDP heuristic: 35.07408973778218\n"
     ]
    }
   ],
   "source": [
    "def test_heuristic(pomdp, initial_state, qfunction, heuristic, n, NRUNS):\n",
    "    \"\"\"\n",
    "    Simulate NRUNS trajectories of length n using a given action-selection heuristic\n",
    "      to update the belief and select actions in a POMDP.\n",
    "\n",
    "    :param pomdp: POMDP description\n",
    "    :type: tuple\n",
    "    :param initial_state: initial state\n",
    "    :type: int\n",
    "    :param qfunction: optimal q-function for the underlying MDP\n",
    "    :type: nd.array\n",
    "    :param heuristic: selected heuristic\n",
    "    :type: str\n",
    "    :param n: length of trajectories\n",
    "    :type: int\n",
    "    :param NRUNS: number of sampled trajectories\n",
    "    :type: int\n",
    "\n",
    "    :returns: float\n",
    "    \"\"\"\n",
    "    X, A, Z, P, O, c, gamma = pomdp\n",
    "\n",
    "    n_states = len(X)\n",
    "    n_actions = len(A)\n",
    "    n_obs = len(Z)\n",
    "\n",
    "    total_costs = []\n",
    "\n",
    "    for _ in range(NRUNS):\n",
    "        # (A) Randomly sample an initial state\n",
    "        s = rnd.choice(n_states)\n",
    "\n",
    "        # (B) Initialize belief to uniform\n",
    "        b = np.ones(n_states) / n_states\n",
    "\n",
    "        # Track discounted cost\n",
    "        discounted_cost = 0.0\n",
    "        discount_factor = 1.0\n",
    "        \n",
    "        for t in range(n):\n",
    "            # select acao pela heuristica\n",
    "            a = get_heuristic_action(b, qfunction, heuristic)\n",
    "\n",
    "            # add cost\n",
    "            discounted_cost += discount_factor * c[s, a]\n",
    "\n",
    "            # sample next state\n",
    "            s_next = rnd.choice(n_states, p=P[a][s])\n",
    "\n",
    "            # sample da obs\n",
    "            z = rnd.choice(n_obs, p=O[a][s_next])\n",
    "\n",
    "            # update do belief\n",
    "            b = belief_update(pomdp, b, a, z)\n",
    "\n",
    "            # Avanço\n",
    "            s = s_next\n",
    "            discount_factor *= gamma\n",
    "\n",
    "        total_costs.append(discounted_cost)\n",
    "\n",
    "    return float(np.mean(total_costs))\n",
    "\n",
    "rand.seed(37)\n",
    "\n",
    "n = 50\n",
    "NRUNS = 100\n",
    "\n",
    "initial_state = 2\n",
    "print(\"initial_state:\", initial_state)\n",
    "\n",
    "mls_mean = test_heuristic(M, initial_state, Q, \"mls\", n, NRUNS)\n",
    "print(\"MLS heuristic:\", mls_mean)\n",
    "\n",
    "av_mean = test_heuristic(M, initial_state, Q, \"av\", n, NRUNS)\n",
    "print(\"AV heuristic:\", av_mean)\n",
    "\n",
    "q_mdp_mean = test_heuristic(M, initial_state, Q, \"q-mdp\", n, NRUNS)\n",
    "print(\"Q-MDP heuristic:\", q_mdp_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXkYcue7YgoJ"
   },
   "source": [
    "As an example, you can run the following code.\n",
    "\n",
    "```python\n",
    "rand.seed(37)\n",
    "\n",
    "n = 50\n",
    "NRUNS = 100\n",
    "\n",
    "initial_state = 2\n",
    "print(\"initial_state:\", initial_state)\n",
    "\n",
    "mls_mean = test_heuristic(M, initial_state, Q, \"mls\", n, NRUNS)\n",
    "print(\"MLS heuristic:\", mls_mean)\n",
    "\n",
    "av_mean = test_heuristic(M, initial_state, Q, \"av\", n, NRUNS)\n",
    "print(\"AV heuristic:\", av_mean)\n",
    "\n",
    "q_mdp_mean = test_heuristic(M, initial_state, Q, \"q-mdp\", n, NRUNS)\n",
    "print(\"Q-MDP heuristic:\", q_mdp_mean)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "initial_state: 2\n",
    "MLS heuristic: 35.72418905862317\n",
    "AV heuristic: 39.499393286246324\n",
    "Q-MDP heuristic: 35.1216686334142\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-43O4rCdJw7Z"
   },
   "source": [
    "<font color=\"blue\">**Question 2:** **Q2.1** Which heuristic(s) appear(s) to perform the best? **Q2.2** Do we have any guarantees on the optimality of these action-selection procedures?\n",
    "Justify. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGRtvwMcF5ao"
   },
   "source": [
    "<font color='blue'>**Insert answer** **Q2.1** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">**Question 2:** **Q2.2** In activity 6 what is the critical point in terms of efficiency?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Insert answer** **Q2.2** </font>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "_PADI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
